{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b601e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "# https://github.com/technoapurva/Steam-Bundle-Recommendation/tree/master\n",
    "\n",
    "# Backup\n",
    "# https://www.kaggle.com/datasets/nikdavis/steam-store-games\n",
    "# https://cseweb.ucsd.edu/~jmcauley/datasets.html#steam_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f46face",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://drive.google.com/drive/folders/1c95okO8Xzrzl4I7S8rWkHZA4FTLY4J45?usp=sharing\n",
    "    \n",
    "items_set=pickle.load(open('data/processed_data/item_set','rb'))\n",
    "bundle_item_map=pickle.load(open('data/processed_data/bundle_item_map','rb'))\n",
    "user_bundle_map=pickle.load(open('data/processed_data/user_bundle_map','rb'))\n",
    "user_item_map=pickle.load(open('data/processed_data/user_item_map','rb'))\n",
    "item_data=pickle.load(open('data/processed_data/all_items','rb'))\n",
    "item_id_lookup = pickle.load(open('data/processed_data/item_id_lookup','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7847f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa um dicionário vazio para mapear 'appid' para os dados do item\n",
    "item_data_map = dict()\n",
    "\n",
    "# Inicializa um conjunto vazio para coletar tags únicas associadas aos itens\n",
    "tags_set = set()\n",
    "\n",
    "# Itera sobre cada item em item_data\n",
    "for item in item_data:\n",
    "    # Converte 'appid' para inteiro e mapeia para o item em item_data_map\n",
    "    item_data_map[int(item['appid'])] = item\n",
    "\n",
    "    # Itera sobre as tags no item atual e as adiciona a tags_set\n",
    "    for tag in item['tags']:\n",
    "        tags_set.add(tag)\n",
    "\n",
    "# Inicializa um dicionário vazio para mapear tags únicas para índices inteiros\n",
    "tags_map = dict()\n",
    "\n",
    "# Atribui índices a cada tag única em tags_set\n",
    "for i, tag in enumerate(tags_set):\n",
    "    tags_map[tag] = i\n",
    "\n",
    "# Define uma função para converter uma lista de tags em uma matriz de características binárias\n",
    "def get_feat(tags):\n",
    "    # Inicializa uma matriz de características binárias com zeros\n",
    "    feat = np.zeros(len(tags_map))\n",
    "\n",
    "    # Define o índice correspondente como 1 para cada tag na lista de entrada\n",
    "    for tag in tags:\n",
    "        feat[tags_map[tag]] = 1\n",
    "\n",
    "    return feat\n",
    "\n",
    "# Inicializa uma lista vazia para armazenar dados combinados de usuários e bundles\n",
    "all_data = []\n",
    "\n",
    "# Itera sobre os itens no mapeamento de usuário para bundles\n",
    "for user, bundles in user_bundle_map.items():\n",
    "    for bundle in bundles:\n",
    "        # Adiciona uma tupla contendo o usuário e o bundle à lista all_data\n",
    "        all_data.append((user, bundle))\n",
    "\n",
    "# Inicializa uma lista vazia para armazenar dados combinados de usuários e itens\n",
    "all_item_data = []\n",
    "\n",
    "# Itera sobre os itens no mapeamento de usuário para itens\n",
    "for user, items in user_item_map.items():\n",
    "    for item in items:\n",
    "        # Adiciona uma tupla contendo o usuário e o item à lista all_item_data\n",
    "        all_item_data.append((user, item))\n",
    "\n",
    "import random\n",
    "\n",
    "# Embaralha aleatoriamente a lista de dados combinados de usuários e bundles\n",
    "random.shuffle(all_data)\n",
    "\n",
    "# Obtém o tamanho total dos dados após o embaralhamento\n",
    "data_size = len(all_data)\n",
    "\n",
    "# Divide os dados embaralhados em conjuntos de treinamento e teste para o modelo BPR para bundles\n",
    "training_data = all_data[:int(0.8 * data_size)]\n",
    "test_data = all_data[int(0.8 * data_size):]\n",
    "\n",
    "# Divide os dados de itens em conjuntos de treinamento e teste para o modelo BPR para itens\n",
    "training_data_2 = all_item_data[:int(0.8 * len(all_item_data))]\n",
    "test_data_2 = all_item_data[int(0.8 * len(all_item_data)):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e891eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tuple(tuple_1, tuple_2, user_bundle_map):\n",
    "    # Verifica se os itens das tuplas não estão presentes nos bundles dos usuários correspondentes\n",
    "    return tuple_1[1] not in user_bundle_map[tuple_2[0]] and tuple_2[1] not in user_bundle_map[tuple_1[0]]\n",
    "\n",
    "def graph_sampling(n_samples, training_data, user_bundle_map):\n",
    "    # Listas para armazenar usuários, itens positivos e itens negativos para amostragem\n",
    "    sgd_users = []\n",
    "    sgd_pos_items, sgd_neg_items = [], []\n",
    "    \n",
    "    i = 0\n",
    "    while n_samples > 0:\n",
    "        if i % 100000 == 0:\n",
    "            print(i)\n",
    "        i += 1\n",
    "        \n",
    "        # Seleciona duas tuplas aleatórias do conjunto de treinamento\n",
    "        tuple_1 = training_data[np.random.randint(len(training_data))]\n",
    "        tuple_2 = training_data[np.random.randint(len(training_data))]\n",
    "        \n",
    "        # Limita o número de iterações para evitar loops infinitos\n",
    "        iteration = 100\n",
    "        \n",
    "        # Enquanto as tuplas não atenderem à condição de verificação, escolhe novas tuplas\n",
    "        while not check_tuple(tuple_1, tuple_2, user_bundle_map):\n",
    "            tuple_2 = training_data[np.random.randint(len(training_data))]\n",
    "            iteration -= 1\n",
    "            \n",
    "            # Se atingir o limite de iterações, interrompe o loop\n",
    "            if iteration == 0:\n",
    "                break\n",
    "        \n",
    "        # Se atingir o limite de iterações, continua para a próxima iteração\n",
    "        if iteration == 0:\n",
    "            continue   \n",
    "        \n",
    "        # Adiciona itens positivos e negativos, assim como usuários correspondentes\n",
    "        sgd_neg_items.append(tuple_2[1])\n",
    "        sgd_pos_items.append(tuple_1[1])\n",
    "        sgd_users.append(tuple_1[0])\n",
    "        \n",
    "        sgd_neg_items.append(tuple_1[1])\n",
    "        sgd_pos_items.append(tuple_2[1])\n",
    "        sgd_users.append(tuple_2[0])\n",
    "        \n",
    "        # Decrementa o número de amostras restantes\n",
    "        n_samples -= 2\n",
    "    \n",
    "    # Retorna as listas resultantes após a amostragem\n",
    "    return sgd_users, sgd_pos_items, sgd_neg_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c55951a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n",
      "3200000\n",
      "3300000\n",
      "3400000\n",
      "3500000\n",
      "3600000\n",
      "3700000\n",
      "3800000\n",
      "3900000\n",
      "4000000\n",
      "4100000\n",
      "4200000\n",
      "4300000\n",
      "4400000\n",
      "4500000\n",
      "4600000\n",
      "4700000\n",
      "4800000\n",
      "4900000\n",
      "5000000\n",
      "5100000\n",
      "5200000\n",
      "5300000\n",
      "5400000\n",
      "5500000\n",
      "5600000\n",
      "5700000\n",
      "5800000\n",
      "5900000\n",
      "6000000\n",
      "6100000\n",
      "6200000\n",
      "6300000\n",
      "6400000\n",
      "6500000\n",
      "6600000\n",
      "6700000\n",
      "6800000\n",
      "6900000\n",
      "7000000\n",
      "7100000\n",
      "7200000\n",
      "7300000\n",
      "7400000\n",
      "7500000\n",
      "7600000\n",
      "7700000\n",
      "7800000\n",
      "7900000\n",
      "8000000\n",
      "8100000\n",
      "8200000\n",
      "8300000\n",
      "8400000\n",
      "8500000\n",
      "8600000\n",
      "8700000\n",
      "8800000\n",
      "8900000\n",
      "9000000\n",
      "9100000\n",
      "9200000\n",
      "9300000\n",
      "9400000\n",
      "9500000\n",
      "9600000\n",
      "9700000\n",
      "9800000\n",
      "9900000\n",
      "10000000\n",
      "10100000\n",
      "10200000\n",
      "10300000\n",
      "10400000\n",
      "10500000\n",
      "10600000\n",
      "10700000\n",
      "10800000\n"
     ]
    }
   ],
   "source": [
    "sgd_train_users_items, sgd_train_pos_items, sgd_train_neg_items = graph_sampling(len(training_data_2)*30, training_data_2, user_item_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ab22b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_test_data_items(test_data, train_data):\n",
    "    # Listas para armazenar usuários, itens positivos e itens negativos para testes\n",
    "    users = []\n",
    "    pos_items = []\n",
    "    neg_items = []\n",
    "\n",
    "    # Converte os dados de treinamento e teste em dicionários\n",
    "    train_dict, train_users, train_items = data_to_dict(train_data)\n",
    "    test_dict, test_users, test_items = data_to_dict(test_data)\n",
    "\n",
    "    z = 0\n",
    "    # Itera sobre os usuários no conjunto de teste\n",
    "    for i, user in enumerate(test_dict.keys()):\n",
    "        if (i % 1000 == 0):\n",
    "            print(i)\n",
    "\n",
    "        # Verifica se o usuário também está no conjunto de treinamento\n",
    "        if user in train_users:\n",
    "            # Itera sobre os itens positivos para o usuário no conjunto de teste\n",
    "            for pos_item in test_dict[user]:\n",
    "                # Verifica se o item positivo está no conjunto de treinamento\n",
    "                if pos_item in train_items:\n",
    "                    # Itera sobre os itens no conjunto de treinamento para gerar itens negativos\n",
    "                    for neg_item in train_items:\n",
    "                        # Verifica se o item negativo não está no conjunto de teste e no conjunto de treinamento\n",
    "                        if neg_item not in test_dict[user] and neg_item not in train_dict[user]:\n",
    "                            # Adiciona usuários, itens positivos e itens negativos às listas\n",
    "                            users.append(user)\n",
    "                            pos_items.append(pos_item)\n",
    "                            neg_items.append(neg_item)\n",
    "\n",
    "    # Retorna as listas resultantes para testes\n",
    "    return users, pos_items, neg_items\n",
    "\n",
    "def data_to_dict(data):\n",
    "    # Converte os dados em um dicionário, onde a chave é o usuário e o valor é uma lista de itens associados a esse usuário\n",
    "    data_dict = defaultdict(list)\n",
    "    items = set()\n",
    "    for (user, item) in data:\n",
    "        data_dict[user].append(item)\n",
    "        items.add(item)\n",
    "    return data_dict, set(data_dict.keys()), items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6db7a70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "test_users_cold, test_pos_items_cold, test_neg_items_cold = get_test_data_items(test_data_2, training_data_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131e440f",
   "metadata": {},
   "source": [
    "# Novo SVD para Steam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71bbffb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter # 0\n",
      "Iter # 1\n",
      "Iter # 2\n",
      "Iter # 3\n",
      "Iter # 4\n",
      "Iter # 5\n",
      "Iter # 6\n",
      "Iter # 7\n",
      "Iter # 8\n",
      "Iter # 9\n",
      "Iter # 10\n",
      "Iter # 11\n",
      "Iter # 12\n",
      "Iter # 13\n",
      "Iter # 14\n",
      "Iter # 15\n",
      "Iter # 16\n",
      "Iter # 17\n",
      "Iter # 18\n",
      "Iter # 19\n",
      "Iter # 20\n",
      "Iter # 21\n",
      "Iter # 22\n",
      "Iter # 23\n",
      "Iter # 24\n",
      "Iter # 25\n",
      "Iter # 26\n",
      "Iter # 27\n",
      "Iter # 28\n",
      "Iter # 29\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def svdpp(train, n_factors, lr=0.05, reg=0.02, miter=10):\n",
    "    global_mean = train['playtime'].mean()\n",
    "    n_users = train['userId'].max() + 1\n",
    "    n_items = train['gameId'].max() + 1\n",
    "    bu = np.zeros(n_users)\n",
    "    bi = np.zeros(n_items)\n",
    "    p = np.random.normal(0.1, 0.1, (n_users, n_factors))\n",
    "    q = np.random.normal(0.1, 0.1, (n_items, n_factors))\n",
    "    Y = np.random.normal(0.1, 0.1, (n_users, n_factors))\n",
    "    error = []\n",
    "\n",
    "    for t in range(miter):\n",
    "        sq_error = 0\n",
    "        for index, row in train.iterrows():\n",
    "            u = row['userId']\n",
    "            i = row['gameId']\n",
    "            r_ui = row['playtime']\n",
    "            Nu = train[train['userId'] == u]['gameId'].values\n",
    "            Y_sum = np.zeros(n_factors)\n",
    "            for j in Nu:\n",
    "                Y_sum = Y_sum + Y[j]\n",
    "            P_plus_y = p[u] + (1 / np.sqrt(len(Nu))) * Y_sum\n",
    "            pred = global_mean + bu[u] + bi[i] + q[i].T @ P_plus_y\n",
    "            e_ui = r_ui - pred\n",
    "            sq_error = sq_error + pow(e_ui, 2)\n",
    "            bu[u] = bu[u] + lr * e_ui\n",
    "            bi[i] = bi[i] + lr * e_ui\n",
    "            for f in range(n_factors):\n",
    "                p[u][f] = p[u][f] + lr * (e_ui * q[i][f] - reg * p[u][f])\n",
    "                temp_uf = q[i][f]\n",
    "                q[i][f] = q[i][f] + lr * (e_ui * P_plus_y[f] - reg * q[i, f])\n",
    "                for j in Nu:\n",
    "                    Y[j][f] = Y[j][f] + lr * (e_ui * (1 / np.sqrt(len(Nu)))) * temp_uf - reg * Y[j, f]\n",
    "        error.append(np.sqrt(sq_error / len(train)))\n",
    "\n",
    "    def recommend_games(user_id, n_recommendations=10):\n",
    "        played_games = train[train['userId'] == user_id]['gameId'].values\n",
    "        predicted_playtimes = global_mean + bu[user_id] + bi + q.T @ (p[user_id] + (1 / np.sqrt(len(played_games))) * Y[played_games].sum(axis=0))\n",
    "        ranked_games = np.argsort(predicted_playtimes)[::-1]\n",
    "        recommended_games = [game_id for game_id in ranked_games if game_id not in played_games][:n_recommendations]\n",
    "        return recommended_games\n",
    "\n",
    "    return recommend_games, error\n",
    "\n",
    "\n",
    "# Observed and unobserved data for each user\n",
    "observed = dict()\n",
    "unobserved = dict()\n",
    "\n",
    "for u in user_bundle_map:\n",
    "    observed[u] = get_item_ids(user_bundle_map[u], u)\n",
    "    unobserved[u] = list(items_set - set(observed[u]))\n",
    "\n",
    "# Training the model\n",
    "item_bias, p, q, error = svdpp(user_bundle_map, items_set, n_factors=10, lr=0.05, reg=0.02, miter=30)\n",
    "\n",
    "# Making predictions using the trained model\n",
    "predictions = recommend_games(observed=observed, all_users=list(user_bundle_map.keys()), p=p, q=q, item_bias=item_bias, N=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f021256",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de3dce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# def svdpp(train, n_factors, lr=0.05, reg=0.02, miter=10):\n",
    "#     global_mean = train['playtime'].mean()\n",
    "#     n_users = train['userId'].max() + 1\n",
    "#     n_items = train['gameId'].max() + 1\n",
    "#     bu = np.zeros(n_users)\n",
    "#     bi = np.zeros(n_items)\n",
    "#     p = np.random.normal(0.1, 0.1, (n_users, n_factors))\n",
    "#     q = np.random.normal(0.1, 0.1, (n_items, n_factors))\n",
    "#     Y = np.random.normal(0.1, 0.1, (n_users, n_factors))\n",
    "#     error = []\n",
    "\n",
    "#     for t in range(miter):\n",
    "#         sq_error = 0\n",
    "#         for index, row in train.iterrows():\n",
    "#             u = row['userId']\n",
    "#             i = row['gameId']\n",
    "#             r_ui = row['playtime']\n",
    "#             Nu = train[train['userId'] == u]['gameId'].values\n",
    "#             Y_sum = np.zeros(n_factors)\n",
    "#             for j in Nu:\n",
    "#                 Y_sum = Y_sum + Y[j]\n",
    "#             P_plus_y = p[u] + (1 / np.sqrt(len(Nu))) * Y_sum\n",
    "#             pred = global_mean + bu[u] + bi[i] + q[i].T @ P_plus_y\n",
    "#             e_ui = r_ui - pred\n",
    "#             sq_error = sq_error + pow(e_ui, 2)\n",
    "#             bu[u] = bu[u] + lr * e_ui\n",
    "#             bi[i] = bi[i] + lr * e_ui\n",
    "#             for f in range(n_factors):\n",
    "#                 p[u][f] = p[u][f] + lr * (e_ui * q[i][f] - reg * p[u][f])\n",
    "#                 temp_uf = q[i][f]\n",
    "#                 q[i][f] = q[i][f] + lr * (e_ui * P_plus_y[f] - reg * q[i, f])\n",
    "#                 for j in Nu:\n",
    "#                     Y[j][f] = Y[j][f] + lr * (e_ui * (1 / np.sqrt(len(Nu)))) * temp_uf - reg * Y[j, f]\n",
    "#         error.append(np.sqrt(sq_error / len(train)))\n",
    "\n",
    "#     def recommend_games(user_id, n_recommendations=10):\n",
    "#         played_games = train[train['userId'] == user_id]['gameId'].values\n",
    "#         predicted_playtimes = global_mean + bu[user_id] + bi + q.T @ (p[user_id] + (1 / np.sqrt(len(played_games))) * Y[played_games].sum(axis=0))\n",
    "#         ranked_games = np.argsort(predicted_playtimes)[::-1]\n",
    "#         recommended_games = [game_id for game_id in ranked_games if game_id not in played_games][:n_recommendations]\n",
    "#         return recommended_games\n",
    "\n",
    "#     return recommend_games, error\n",
    "\n",
    "\n",
    "# # Observed and unobserved data for each user\n",
    "# observed = dict()\n",
    "# unobserved = dict()\n",
    "\n",
    "# for u in user_bundle_map:\n",
    "#     observed[u] = get_item_ids(user_bundle_map[u], u)\n",
    "#     unobserved[u] = list(items_set - set(observed[u]))\n",
    "\n",
    "# # Training the BPRMF model\n",
    "# item_bias, p, q, error = svdpp(user_bundle_map, items_set, n_factors=10, lr=0.05, reg=0.02, miter=30)\n",
    "\n",
    "# # Making predictions using the trained model\n",
    "# predictions = recommend_games(observed=observed, all_users=list(user_bundle_map.keys()), p=p, q=q, item_bias=item_bias, N=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73a533a",
   "metadata": {},
   "source": [
    "# SVD Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b58b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def svdpp(train, n_factors, lr=0.05, reg=0.02, miter=10):\n",
    "#     global_mean = train['rating'].mean()\n",
    "#     n_users = df['userId'].max()+1\n",
    "#     n_items = df['movieId'].max()+1\n",
    "#     bu = np.zeros(n_users)\n",
    "#     bi = np.zeros(n_items)\n",
    "#     p = np.random.normal(0.1, 0.1, (n_users, n_factors))\n",
    "#     q = np.random.normal(0.1, 0.1, (n_items, n_factors))\n",
    "#     Y = np.random.normal(0.1, 0.1, (n_users, n_factors))\n",
    "#     error = []\n",
    "#     for t in range(miter):\n",
    "#         sq_error = 0\n",
    "#         for index, row in train.iterrows():\n",
    "#             u = row['userId']\n",
    "#             i = row['movieId']\n",
    "#             r_ui = row['rating']\n",
    "#             Nu = train[train['userId'] == u]['movieId'].values\n",
    "#             Y_sum = np.zeros(n_factors)\n",
    "#             for j in Nu:\n",
    "#                 Y_sum = Y_sum + Y[j]\n",
    "#             P_plus_y = p[u] + (1 /np.sqrt(len(Nu))) * Y_sum\n",
    "#             pred = global_mean + bu[u] + bi[i] + q[i].T @ P_plus_y\n",
    "#             e_ui = r_ui - pred\n",
    "#             sq_error = sq_error + pow(e_ui, 2)\n",
    "#             bu[u] = bu[u] + lr * e_ui\n",
    "#             bi[i] = bi[i] + lr * e_ui\n",
    "#             for f in range(n_factors):\n",
    "#                 p[u][f] = p[u][f] + lr * (e_ui * q[i][f] - reg * p[u][f])\n",
    "#                 temp_uf = q[i][f]\n",
    "#                 q[i][f] = q[i][f] + lr * (e_ui * P_plus_y[f] - reg * q[i, f])\n",
    "#                 for j in Nu:\n",
    "#                     Y[j][f] = Y[j][f] + lr * (e_ui *(1/np.sqrt(len(Nu)))) * temp_uf - reg * Y[j,f]\n",
    "#         error.append(np.sqrt(sq_error/len(train)))\n",
    "\n",
    "#     return global_mean, bu, bi, P_plus_y, p, q, error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a82463e",
   "metadata": {},
   "source": [
    "# BPRMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ec32d65",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter # 0\n",
      "Iter # 1\n",
      "Iter # 2\n",
      "Iter # 3\n",
      "Iter # 4\n",
      "Iter # 5\n",
      "Iter # 6\n",
      "Iter # 7\n",
      "Iter # 8\n",
      "Iter # 9\n",
      "Iter # 10\n",
      "Iter # 11\n",
      "Iter # 12\n",
      "Iter # 13\n",
      "Iter # 14\n",
      "Iter # 15\n",
      "Iter # 16\n",
      "Iter # 17\n",
      "Iter # 18\n",
      "Iter # 19\n",
      "Iter # 20\n",
      "Iter # 21\n",
      "Iter # 22\n",
      "Iter # 23\n",
      "Iter # 24\n",
      "Iter # 25\n",
      "Iter # 26\n",
      "Iter # 27\n",
      "Iter # 28\n",
      "Iter # 29\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to get items associated with a user in the training set\n",
    "def get_item_ids(bundle_items, user):\n",
    "    return list(bundle_items)\n",
    "\n",
    "# Function to draw a pair of items (i, j) for a specific user\n",
    "def draw(userId, observed):\n",
    "    i = random.choice(observed)\n",
    "    j = random.choice(list(items_set - set(observed)))\n",
    "    return i, j\n",
    "\n",
    "# BPRMF - Training the BPRMF model\n",
    "def train_bprmf(user_bundle_map, item_set, n_factors, lr=0.05, reg=0.02, miter=30):\n",
    "    n_users = len(user_bundle_map)\n",
    "    n_items = len(item_set)\n",
    "    item_bias = np.zeros(n_items)\n",
    "    p = np.random.normal(0, 0.1, (n_users, n_factors))\n",
    "    q = np.random.normal(0, 0.1, (n_items, n_factors))\n",
    "\n",
    "    error = []\n",
    "    for t in range(miter):\n",
    "        print('Iter #', t)\n",
    "        sq_error = 0\n",
    "        random_users = random.sample(list(user_bundle_map.keys()), k=len(user_bundle_map))\n",
    "        for u in random_users:\n",
    "            observed[u] = get_item_ids(user_bundle_map[u], u)\n",
    "            i, j = draw(u, observed[u])\n",
    "\n",
    "            x_uij = item_bias[i] - item_bias[j] + (np.dot(p[u], q[i]) - np.dot(p[u], q[j]))\n",
    "            sq_error += x_uij\n",
    "\n",
    "            eps = 1 / (1 + np.exp(x_uij))\n",
    "\n",
    "            item_bias[i] += lr * (eps - reg * item_bias[i])\n",
    "            item_bias[j] += lr * (-eps - reg * item_bias[j])\n",
    "\n",
    "            # Adjust the factors\n",
    "            u_f = p[u]\n",
    "            i_f = q[i]\n",
    "            j_f = q[j]\n",
    "\n",
    "            # Compute and apply factor updates\n",
    "            p[u] += lr * ((i_f - j_f) * eps - reg * u_f)\n",
    "            q[i] += lr * (u_f * eps - reg * i_f)\n",
    "            q[j] += lr * (-u_f * eps - reg * j_f)\n",
    "\n",
    "        error.append(sq_error / len(random_users))\n",
    "\n",
    "    return item_bias, p, q, error\n",
    "\n",
    "# Function to make predictions using the trained model\n",
    "def predict(observed, all_users, p, q, item_bias, N=10):\n",
    "    w = item_bias.T + np.dot(p, q.T)\n",
    "    ranking = []\n",
    "\n",
    "    for u, user in enumerate(all_users):\n",
    "        partial_ranking = list()\n",
    "        candidate_items = sorted(range(len(w[u])), key=lambda k: w[u][k], reverse=True)\n",
    "\n",
    "        for i in candidate_items:\n",
    "            if i not in observed[user]:\n",
    "                partial_ranking.append((user, i, w[u][i]))\n",
    "\n",
    "            if len(partial_ranking) == N:\n",
    "                break\n",
    "\n",
    "        ranking += partial_ranking\n",
    "\n",
    "    return ranking\n",
    "\n",
    "# Observed and unobserved data for each user\n",
    "observed = dict()\n",
    "unobserved = dict()\n",
    "\n",
    "for u in user_bundle_map:\n",
    "    observed[u] = get_item_ids(user_bundle_map[u], u)\n",
    "    unobserved[u] = list(items_set - set(observed[u]))\n",
    "\n",
    "# Training the BPRMF model\n",
    "item_bias, p, q, error = train_bprmf(user_bundle_map, items_set, n_factors=10, lr=0.05, reg=0.02, miter=30)\n",
    "\n",
    "# Making predictions using the trained model\n",
    "predictions = predict(observed=observed, all_users=list(user_bundle_map.keys()), p=p, q=q, item_bias=item_bias, N=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7ff50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Passando os dados para um arquivo\n",
    "# train.to_csv('train.dat', index=False, header=False, sep='\\t')\n",
    "# test.to_csv('test.dat', index=False, header=False, sep='\\t')\n",
    "\n",
    "\n",
    "# observed = dict()\n",
    "# unobserved = dict()\n",
    "# all_users = df['userId'].unique().tolist() # usar conj. total\n",
    "# all_items = df['itemId'].unique().tolist() # usar conj. total\n",
    "\n",
    "# for u in all_users:\n",
    "#     observed[u] = get_item_ids(train, u) # usar conj. de treinamento\n",
    "#     unobserved[u] = list(set(all_items)-set(observed[u]))\n",
    "\n",
    "# def draw(userId):\n",
    "#     i = random.choice(observed[userId])\n",
    "#     j = random.choice(unobserved[userId])\n",
    "#     return i, j\n",
    "\n",
    "# # BPRMF\n",
    "# def train_bprmf(train, n_factors, lr=0.05, reg=0.02, miter=30):\n",
    "#     n_users = df['userId'].max()+1\n",
    "#     n_items = df['itemId'].max()+1\n",
    "#     item_bias = np.zeros(n_items)\n",
    "#     p = np.random.normal(0, 0.1, (n_users, n_factors))\n",
    "#     q = np.random.normal(0, 0.1, (n_items, n_factors))\n",
    "\n",
    "#     error = []\n",
    "#     for t in range(miter):\n",
    "#         print('Iter #', t)\n",
    "#         sq_error = 0\n",
    "#         random_users = random.choices(train['userId'].unique(), k=len(train))\n",
    "#         for u in random_users:\n",
    "#             i, j = draw(u)\n",
    "#             x_uij = item_bias[i] - item_bias[j] + (np.dot(p[u], q[i]) - np.dot(p[u], q[j]))\n",
    "#             sq_error += x_uij\n",
    "\n",
    "#             eps = 1 / (1 + np.exp(x_uij))\n",
    "\n",
    "#             item_bias[i] += lr * (eps - reg * item_bias[i])\n",
    "#             item_bias[j] += lr * (-eps - reg * item_bias[j])\n",
    "\n",
    "#             # Adjust the factors\n",
    "#             u_f = p[u]\n",
    "#             i_f = q[i]\n",
    "#             j_f = q[j]\n",
    "\n",
    "#             # Compute and apply factor updates\n",
    "#             p[u] += lr * ((i_f - j_f) * eps - reg * u_f)\n",
    "#             q[i] += lr * (u_f * eps - reg * i_f)\n",
    "#             q[j] += lr * (-u_f * eps - reg * j_f)\n",
    "\n",
    "#         error.append(sq_error/len(random_users))\n",
    "\n",
    "#     return item_bias, p, q, error\n",
    "\n",
    "# def predict(N=10):\n",
    "#     w = b.T + np.dot(p, q.T)\n",
    "#     ranking = []\n",
    "\n",
    "#     for u, user in enumerate(all_users):\n",
    "#         partial_ranking = list()\n",
    "#         candidate_items = sorted(range(len(w[u])), key=lambda k: w[u][k], reverse=True)\n",
    "\n",
    "#         for i in candidate_items:\n",
    "#             if i not in observed[user]:\n",
    "#                 partial_ranking.append((user, i, w[u][i]))\n",
    "\n",
    "#             if len(partial_ranking) == N:\n",
    "#                 break\n",
    "\n",
    "#         ranking += partial_ranking\n",
    "\n",
    "#     return pd.DataFrame(ranking, columns=['userId', 'movieId', 'score'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
